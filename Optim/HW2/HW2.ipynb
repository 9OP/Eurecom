{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optim HW 2 : Gradient methods\n",
    "___\n",
    "Martin Guyard\n",
    "\n",
    "**Due date: 20 Nov. 2019**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to minimize unconstrained $f$:\n",
    "\n",
    "$$ f(x_1,x_2,...,x_m) = \\sum_{i=1}^{m} a_1\\cdot(x_i-b_i)^2 + 3 $$\n",
    " \n",
    "$$ x_{i}^{k+1} = x_{i}^{k} - t\\nabla{f(x_{i}^{k})} $$\n",
    "$$ x_{i}^{k+1} = x_{i}^{k} - t\\cdot2\\cdot a_i(x_{i}^{k} - b_i) $$\n",
    "\n",
    "___\n",
    "**Analytical solution**:\n",
    "\n",
    "$f$ is minimum when $\\forall i, x_i = b_i$. Moreover we can solve this problem with gradient descent in one step with:\n",
    "$\\forall i, x_{i}^{k} - t\\cdot2\\cdot a_i(x_{i}^{k} - b_i) = b_i $, means that $\\forall{i},  2\\cdot t \\cdot a_i = 1$.\n",
    "\n",
    "**One trivial solution is $t=0.5$ and $\\forall i, a_i=1$**\n",
    "___\n",
    "**Convergence analysis**:\n",
    "\n",
    "The gradient descent algorithm converge on $t<1/L$ given that $f$ is convex and $L$ Lipschitz.\n",
    "\n",
    "$f$ is $L$ Lipschitz if $\\| \\nabla f(x) - \\nabla f(y)\\|_2 \\leq L\\|x-y\\|_2$\n",
    "\n",
    "\n",
    "$\\|A(X-B) - A(Y-b)\\|_2 \\leq L \\|X-Y\\|_2$\n",
    "\n",
    "$\\|AX - AY\\|_2 \\leq L \\|X-Y\\|_2$\n",
    "\n",
    "$\\|A(X-Y)\\|_2 \\leq L \\|X-Y\\|_2$\n",
    "\n",
    "$max(A)\\|X-Y\\|_2 \\leq L \\|X-Y\\|_2$\n",
    "\n",
    "$L \\geq max(A)$\n",
    "\n",
    "**if $\\forall i, a_i=1$, then the convergence criterium is $L \\geq 1$ and $t \\leq 1$**\n",
    "\n",
    "Gradient (and gradient+backtracking) convergence rate is $O(1/k)$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from random import randrange\n",
    "import numpy as np\n",
    "\n",
    "def norm(K, L=None):\n",
    "    # return euclidean distance between vector K and L\n",
    "    if L:\n",
    "        X = [k-l for k, l  in zip(K, L)]\n",
    "    else:\n",
    "        X = K\n",
    "    return sqrt(sum(x**2 for x in X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad is a lambda function that compute the gradient of f\n",
    "# update is a lambda function that compute the descent: x(k+1)\n",
    "\n",
    "def gradient_descent(X, grad, update, epsilon=1e-6, step=0.5):\n",
    "    gradf = grad\n",
    "    descent = update\n",
    "    \n",
    "    X_k = list(X)\n",
    "    X_k1 = descent(X_k, step)\n",
    "    \n",
    "    steps = 0\n",
    "    while norm(X_k, X_k1) >= epsilon:\n",
    "        X_k2 = descent(X_k1, step)\n",
    "        X_k = list(X_k1)\n",
    "        X_k1 = list(X_k2)\n",
    "        steps += 1\n",
    "        \n",
    "    print(\"Gradient descent steps:\", steps)\n",
    "    print(\"[ step size t =\", step, ', convergence criterium 1/t <', max(A),']\\n')\n",
    "    return X_k   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with Backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_bt(X, func, grad, update, epsilon=1e-6, step=0.5, alpha=0.5, beta=0.5, verbose=True):\n",
    "    f = func\n",
    "    gradf = grad\n",
    "    descent = update\n",
    "\n",
    "    X_k = list(X)\n",
    "    X_k1 = descent(X_k, step)\n",
    "    \n",
    "    steps = 0\n",
    "    while norm(X_k, X_k1) >= epsilon:\n",
    "        while f(X_k1) > f(X_k) - alpha * step * norm(gradf(X_k)) ** 2:\n",
    "            step *= beta\n",
    "            X_k1 = descent(X_k, step)\n",
    "        else:\n",
    "            X_k2 = descent(X_k1, step)\n",
    "            X_k = list(X_k1)\n",
    "            X_k1 = list(X_k2)\n",
    "        steps += 1\n",
    "    \n",
    "    if verbose:\n",
    "        print('Gradient descent with backtracking steps:', steps)\n",
    "        print('[ step size t =', step, ', alpha =', alpha, ', beta =', beta, ']\\n')\n",
    "    return X_k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Assume that:\n",
    "\n",
    "- $ m = 500 $\n",
    "\n",
    "- $ \\forall i, a_i = 1 $\n",
    "\n",
    "- $ \\forall i, b_i \\sim U(0, 100) $\n",
    "\n",
    "- $ \\forall i, x_i = 0 $ for initial x\n",
    "\n",
    "**As we already analyticaly solved this precise problem, I choose step size t = 1**\n",
    "\n",
    "**The stopping condition is $d(x_{k+1}, x_{k}) \\leq \\varepsilon$ with default $\\varepsilon = 1e^{-6}$ and $d(.)$ the euclidean distance**\n",
    "\n",
    "**Note: The stopping condition could also be X == B or norm(f_grad(X)) <= epsilon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 500\n",
    "X = [0] * m\n",
    "A = [1] * m\n",
    "B = [randrange(0, 100) for i in range(m)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!!The code is made to be reused!!!**\n",
    "\n",
    "f, gradf and descent are python lambda function. These function are not harcoded into gradient_descent(...) and gradient_descent_with_backtracking(...). This makes easier code reuse for problem2.\n",
    "\n",
    "**recall:** \n",
    "\n",
    "f: $f(x_1,x_2,...,x_m) = \\sum_{i=1}^{m} a_i\\cdot(x_i-b_i)^2 + 3$\n",
    "\n",
    "gradf: $\\nabla f(x_i) = 2a_i\\cdot(x_i-b_i)$, **note:** that gradf returns a vector of $(\\nabla f(x_1), ...,\\nabla f(x_m))$\n",
    "\n",
    "decent: $x_{i}^{k+1} = x_{i}^{k} - t\\cdot2\\cdot a_i(x_{i}^{k} - b_i)$, **note:** that descent returns a vector of $(x_1^{k+1}, ...,x_m^{k+1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda X: sum([(a*(x-b)**2) for x, a, b in zip(X, A, B)])+3 # compute f(X)\n",
    "gradf = lambda X: [2*a*(x-b) for x, a, b in zip(X, A, B)] # compute the grad vector: ouput = list\n",
    "descent = lambda X, t: [x-t*grdf for x, grdf in zip(X, gradf(X))] # compute the update vector: output = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent steps: 1\n",
      "[ step size t = 0.5 , convergence criterium 1/t < 1 ]\n",
      "\n",
      "Gradient descent with backtracking steps: 20\n",
      "[ step size t = 0.328125 , alpha = 0.5 , beta = 0.5 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gd_res = gradient_descent(X, grad=gradf, update=descent, step=0.5)\n",
    "\n",
    "# step size don't batter for backtracking because of adaptative step size\n",
    "gd_bt_res = gradient_descent_bt(X, func=f, grad=gradf, update=descent, step=42)\n",
    "\n",
    "# print(gd_res)\n",
    "# print(gd_bt_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The convergence speed is the same with step size t=0.5 since this solve the problem analytically as shown above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Assume that:\n",
    "\n",
    "- $ m = 500 $\n",
    "\n",
    "- $ \\forall i, a_i \\sim U(1, 100) $\n",
    "\n",
    "- $ \\forall i, b_i \\sim U(1, 100) $\n",
    "\n",
    "- $ \\forall i, x_i = 0 $ for initial x\n",
    "\n",
    "**I choose step size t=1/100 since max(A) < 100**\n",
    "\n",
    "**The stopping condition is $d(x_{k+1}, x_{k}) \\leq \\varepsilon$ with default $\\varepsilon = 1e^{-6}$ and $d(.)$ the euclidean distance**\n",
    "\n",
    "**Note: The stopping condition could also norm(f_grad(X)) <= epsilon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 500\n",
    "X = [0] * m\n",
    "A = [randrange(1, 100) for i in range(m)]\n",
    "B = [randrange(1, 100) for i in range(m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent steps: 944\n",
      "[ step size t = 0.01 , convergence criterium 1/t < 99 ]\n",
      "\n",
      "Gradient descent with backtracking steps: 1233\n",
      "[ step size t = 0.005126953125 , alpha = 0.5 , beta = 0.5 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gd_res = gradient_descent(X, grad=gradf, update=descent, step=1/100)\n",
    "\n",
    "# step size don't batter for backtracking because of adaptative step size\n",
    "gd_bt_res = gradient_descent_bt(X, func=f, grad=gradf, update=descent, step=42)\n",
    "\n",
    "# print(gd_res)\n",
    "# print(gd_bt_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The convergence speed is '*usually*' slower with backtracking as it is taking more steps, however backtracking advantage is not to converge faster, but to always converge, with batcktracking step size can be anything since the backtrack is updating the step size.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to **minimize** **constrained** $f$:\n",
    "\n",
    "$$ f(x_1,x_2,...,x_m) = \\sum_{i=1}^{m} a_1\\cdot(x_i-b_i)^2 + 3 $$\n",
    "$$\\forall i, x_i \\geq 0 $$\n",
    "$$\\sum_{i=0}^{m} x_i \\leq 100 $$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Trying to find an anaytical solution\n",
    "\n",
    "This problem has no equalities constraints, the Lagrangian is simply:\n",
    "<br>$L(x, \\lambda) = f(x) + \\lambda h(x)$ where $x=(x_1, ... , x_m)$ and $\\lambda=(\\lambda_1, ..., \\lambda_{m+1})$\n",
    "<br>\n",
    "$h_{m+1}(x)=(\\sum_{i=1}^{m} x_i) -100 \\leq 0$\n",
    "<br>\n",
    "$\\forall i \\in [1, m], h_{i}(x)=-x \\leq 0$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**KKT conditions:**\n",
    "- primal feasibility: $\\forall i, -x_i \\leq 0$ and $\\sum_{i=1}^m x_i -100 \\leq 0$\n",
    "- dual feasibility: $\\forall i \\in [1, m+1], \\lambda_i \\geq 0$ and $\\forall i \\in [1,m], \\lambda_i (-x_i) = 0$\n",
    "- complementary slackness: $\\forall i \\in [1,m], x_i(2a_i(x_i-b_i)-\\lambda_i + \\lambda_{m+1})=0$ and $\\lambda_{m+1} (\\sum_{i=1}^{m} x_i - 100) = 0$\n",
    "\n",
    "\n",
    "<br>\n",
    "We consider $a_i, b_i \\geq 1$\n",
    "<br>\n",
    "<br>\n",
    "With complementary slackness, $xi=b_i+\\frac{\\lambda_i-\\lambda_{m+1}}{2a_i}\\geq0$ \n",
    "\n",
    "**Since $x_i \\geq 0$ we got $x_i = max(0, b_i+\\frac{\\lambda_i-\\lambda_{m+1}}{2a_i})$**\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Then $\\lambda_{m+1} (\\sum_{i=1}^{m} x_i - 100) = 0$ implies that either $\\lambda_{m+1} = 0$ or $\\sum_{i=1}^{m} x_i - 100=0$\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DUAL ASCENT\n",
    "___\n",
    "$min f(x)$ constrained to\n",
    "$\\sum x_i -100 \\leq 0$ and $x_i \\geq 0$\n",
    "\n",
    "**PRIMAL:** $min_x max_\\lambda (f(x) +\\lambda h(x))$\n",
    "\n",
    "**DUAL:** $max_\\lambda min_x (f(x) +\\lambda h(x))$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "dual ascent update:\n",
    "\n",
    "$x^{k+1} = min_x(L(x^{k}, \\lambda^{k}))$ **Note: we are going to use our previous implementation of gradient decent with backtracking to compute this**\n",
    "\n",
    "$\\lambda^{k+1}=\\lambda^{k} - t(\\sum x_i^{k+1} - 100)$, t is a fixed step in this example (choosen)\n",
    "\n",
    "For the dual ascent to work, the first step is a minimization of the Lagrangian, we can use Gradient Descent Backtracking to ensure that we found a X*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 500\n",
    "X = [0] * m\n",
    "A = [randrange(1, 100) for i in range(m)]\n",
    "B = [randrange(1, 100) for i in range(m)]\n",
    "U = [0] * (m+1) # we have m+1 dual mutlipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent with backtracking steps: 1728\n",
      "[ step size t = 0.00390625 , alpha = 0.5 , beta = 0.5 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute f\n",
    "f = lambda X: sum([(a*(x-b)**2) for x, a, b in zip(X, A, B)])+3 # compute f(X)\n",
    "\n",
    "# compute Lagrangian of f\n",
    "lagrangian = lambda X: sum([(a*(x-b)**2) for x, a, b in zip(X, A, B)]) + 3 +\\\n",
    "             sum([(-x)*u for x, u in zip(X, U[:-1])]) + U[-1] * sum(X)\n",
    "\n",
    "# compute for all i, grad(lagrangian(x_i))\n",
    "gradL = lambda X: [2*a*(x-b)-u+U[-1] for x, a, b, u in zip(X, A, B, U[:-1])]\n",
    "\n",
    "# update for all i, x_i(k+1) = x_i(k) - t*grad(lagrangian(x_i(k)))\n",
    "descentL = lambda X, t: [x-t*grdf for x, grdf in zip(X, gradL(X))]\n",
    "\n",
    "def descentU(U, X, t):\n",
    "    U_k1 = list()\n",
    "    for u, x in zip(U[:-1], X):\n",
    "        U_k1.append(max(u+t*(u*x), 0))\n",
    "    U_k1.append(max(U[-1]+t*(sum(X)-100), 0))\n",
    "    return U_k1\n",
    "\n",
    "gd_bt_lagrangian = gradient_descent_bt(X, func=lagrangian, grad=gradL, update=descentL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(X) =  21120.7646895289 xi > 0 : False\n",
      "sum(X) =  18395.484273235914 xi > 0 : False\n",
      "sum(X) =  16023.528688686356 xi > 0 : False\n",
      "sum(X) =  13959.089970812107 xi > 0 : False\n",
      "sum(X) =  12162.299671105657 xi > 0 : False\n",
      "sum(X) =  10598.457900668873 xi > 0 : False\n",
      "sum(X) =  9237.363605582195 xi > 0 : False\n",
      "sum(X) =  8052.731461713046 xi > 0 : False\n",
      "sum(X) =  7021.683109460639 xi > 0 : False\n",
      "sum(X) =  6124.307187981797 xi > 0 : False\n",
      "sum(X) =  5343.273389114252 xi > 0 : False\n",
      "sum(X) =  4663.498401979053 xi > 0 : False\n",
      "sum(X) =  4071.8542649649726 xi > 0 : False\n",
      "sum(X) =  3556.9152758379723 xi > 0 : False\n",
      "sum(X) =  3108.736509169715 xi > 0 : False\n",
      "sum(X) =  2718.6627488067015 xi > 0 : False\n",
      "sum(X) =  2379.1605204945986 xi > 0 : False\n",
      "sum(X) =  2083.674173949245 xi > 0 : False\n",
      "sum(X) =  1826.496615656044 xi > 0 : False\n",
      "sum(X) =  1602.6614520111918 xi > 0 : False\n",
      "sum(X) =  1407.8458192856915 xi > 0 : False\n",
      "sum(X) =  1238.2874742504082 xi > 0 : False\n",
      "sum(X) =  1090.7122596553106 xi > 0 : False\n",
      "sum(X) =  962.269026947729 xi > 0 : False\n",
      "sum(X) =  850.4785551448721 xi > 0 : False\n",
      "sum(X) =  753.1808424885184 xi > 0 : False\n",
      "sum(X) =  668.497954742279 xi > 0 : False\n",
      "sum(X) =  594.7936661521065 xi > 0 : False\n",
      "sum(X) =  530.6456603256117 xi > 0 : False\n",
      "sum(X) =  474.8136214893638 xi > 0 : False\n",
      "sum(X) =  426.2196269660877 xi > 0 : False\n",
      "sum(X) =  383.92629784041674 xi > 0 : False\n",
      "sum(X) =  347.1160107846012 xi > 0 : False\n",
      "sum(X) =  315.07841061309625 xi > 0 : False\n",
      "sum(X) =  287.19410066679063 xi > 0 : False\n",
      "sum(X) =  262.92471606539874 xi > 0 : False\n",
      "sum(X) =  241.80256708652115 xi > 0 : False\n",
      "sum(X) =  223.4178818850997 xi > 0 : False\n",
      "sum(X) =  207.41723508979874 xi > 0 : False\n",
      "sum(X) =  193.49108219507576 xi > 0 : False\n",
      "sum(X) =  181.3697510944405 xi > 0 : False\n",
      "sum(X) =  170.8204072168194 xi > 0 : False\n",
      "sum(X) =  161.63903189961442 xi > 0 : False\n",
      "sum(X) =  153.647650138792 xi > 0 : False\n",
      "sum(X) =  146.69240769759412 xi > 0 : False\n",
      "sum(X) =  140.63883507713302 xi > 0 : False\n",
      "sum(X) =  135.37023772958827 xi > 0 : False\n",
      "sum(X) =  130.78456527131846 xi > 0 : False\n",
      "sum(X) =  126.79366595214955 xi > 0 : False\n",
      "sum(X) =  123.31974560915384 xi > 0 : False\n",
      "sum(X) =  120.29638144043872 xi > 0 : False\n",
      "sum(X) =  117.66508906903971 xi > 0 : False\n",
      "sum(X) =  115.37444951047252 xi > 0 : False\n",
      "sum(X) =  113.38146074039474 xi > 0 : False\n",
      "sum(X) =  111.64672852870095 xi > 0 : False\n",
      "sum(X) =  110.13657610546804 xi > 0 : False\n",
      "sum(X) =  108.82278789677551 xi > 0 : False\n",
      "sum(X) =  107.67907669929781 xi > 0 : False\n",
      "sum(X) =  106.6829259720187 xi > 0 : False\n",
      "sum(X) =  105.81673510595917 xi > 0 : False\n",
      "sum(X) =  105.06283506064618 xi > 0 : False\n",
      "sum(X) =  104.40647044209734 xi > 0 : False\n",
      "sum(X) =  103.83482220529962 xi > 0 : False\n",
      "sum(X) =  103.33771936747382 xi > 0 : False\n",
      "sum(X) =  102.90543363621589 xi > 0 : False\n",
      "sum(X) =  102.52842375619738 xi > 0 : False\n",
      "sum(X) =  102.20053793743767 xi > 0 : False\n",
      "sum(X) =  101.91534696980548 xi > 0 : False\n",
      "sum(X) =  101.66696754064264 xi > 0 : False\n",
      "sum(X) =  101.45159289485491 xi > 0 : False\n",
      "sum(X) =  101.26291841552526 xi > 0 : False\n",
      "sum(X) =  101.09896935098772 xi > 0 : False\n",
      "sum(X) =  100.95708814701175 xi > 0 : False\n",
      "sum(X) =  100.83378492475974 xi > 0 : False\n",
      "sum(X) =  100.72434235994804 xi > 0 : False\n",
      "sum(X) =  100.63145469183411 xi > 0 : False\n",
      "sum(X) =  100.54855758858673 xi > 0 : False\n",
      "sum(X) =  100.4778544808417 xi > 0 : False\n",
      "sum(X) =  100.41601091457565 xi > 0 : False\n",
      "sum(X) =  100.36225011703385 xi > 0 : False\n",
      "sum(X) =  100.315394245719 xi > 0 : False\n",
      "sum(X) =  100.27416775404632 xi > 0 : False\n",
      "sum(X) =  100.23889749647678 xi > 0 : False\n",
      "sum(X) =  100.2077308595923 xi > 0 : False\n",
      "sum(X) =  100.18191415110093 xi > 0 : False\n",
      "sum(X) =  100.15738234763131 xi > 0 : False\n",
      "sum(X) =  100.13843335199147 xi > 0 : False\n",
      "sum(X) =  100.11912263595377 xi > 0 : False\n",
      "sum(X) =  100.10395828515135 xi > 0 : False\n",
      "sum(X) =  100.09052930143382 xi > 0 : False\n",
      "sum(X) =  100.07901168589154 xi > 0 : False\n",
      "sum(X) =  100.06846896931606 xi > 0 : False\n",
      "sum(X) =  100.06061096291833 xi > 0 : False\n",
      "sum(X) =  100.05370437387339 xi > 0 : False\n",
      "sum(X) =  100.0445998020974 xi > 0 : False\n",
      "sum(X) =  100.04107044261467 xi > 0 : False\n",
      "sum(X) =  100.03491788783856 xi > 0 : False\n",
      "sum(X) =  100.0304043329626 xi > 0 : False\n",
      "sum(X) =  100.02683478626398 xi > 0 : False\n",
      "sum(X) =  100.0227075887496 xi > 0 : False\n",
      "sum(X) =  100.02063796654724 xi > 0 : False\n",
      "sum(X) =  100.01870770148635 xi > 0 : False\n",
      "sum(X) =  100.01602562595565 xi > 0 : False\n",
      "sum(X) =  100.0125327538275 xi > 0 : False\n",
      "sum(X) =  100.01062577127253 xi > 0 : False\n",
      "sum(X) =  100.00957742371631 xi > 0 : False\n",
      "sum(X) =  100.00866884381173 xi > 0 : False\n",
      "sum(X) =  100.0071388448741 xi > 0 : False\n",
      "sum(X) =  100.00606034001936 xi > 0 : False\n",
      "sum(X) =  100.00598301556792 xi > 0 : False\n",
      "sum(X) =  100.00598301556832 xi > 0 : False\n",
      "Dual ascent steps: 111\n",
      "[ sum(X*) = 100.00598301556832 , xi > 0 : False ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dual_ascent(X, updateU, epsilon=1e-6, step=0.01):\n",
    "    global U\n",
    "    \n",
    "    descentU = updateU\n",
    "    \n",
    "    X_k = list(X)\n",
    "    X_k1 = gradient_descent_bt(X_k, func=lagrangian, grad=gradL, update=descentL,verbose=False)\n",
    "    \n",
    "    U_k = list(U)\n",
    "    U_k1 = descentU(U_k, X_k1, step)\n",
    "    U = list(U_k1)\n",
    "     \n",
    "    steps = 0\n",
    "    while norm(X_k, X_k1) >= epsilon:\n",
    "        X_k2 = gradient_descent_bt(X_k1, func=lagrangian, grad=gradL, update=descentL, verbose=False)\n",
    "        U_k2 = descentU(U_k1, X_k2, step)\n",
    "        \n",
    "        X_k = list(X_k1)\n",
    "        X_k1 = list(X_k2)\n",
    "        \n",
    "        U_k = list(U_k1)\n",
    "        U_k1 = list(U_k2)\n",
    "        U = list(U_k1) # IMPORTANT: update global variable\n",
    "        \n",
    "        steps += 1\n",
    "        print(\"sum(X) = \", sum(X_k1), 'xi > 0 :', all(x >= 30 for x in X_k1))\n",
    "    \n",
    "    print('Dual ascent steps:', steps)\n",
    "    print('[ sum(X*) =', sum(X_k1), ', xi > 0 :', all(x >= 30 for x in X_k1), ']\\n')\n",
    "    return X_k1\n",
    "                                   \n",
    "res = dual_ascent(X, updateU=descentU)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopping condition**: same as gradient descent, when the distance between $x^{k}$ and $x^{k+1}$ is less than epsilon it means that the algorithm has converge to a solution.\n",
    "\n",
    "**Step size**: I tried several, this one worked.\n",
    "\n",
    "**Convergence rate**: O(1/k) k: number of steps\n",
    "\n",
    "**The algorithm could be paralellize:** \n",
    "\n",
    "1thread compute the grad of L, 1thread compute the new dual multiplier, it would divide by a factor 2 the number of iteration to convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "___\n",
    "Dual Ascent was really hard for me to understand, here are ressources I found online to help me better understand the principle, tho I am still not sure that I really get it.\n",
    "\n",
    "**dual ascent YT tutorial: https://www.youtube.com/watch?v=HOx-fZ01VnY**\n",
    "\n",
    "**exercise: https://bdesgraupes.pagesperso-orange.fr/UPX/Master1/MNM1_corr_doc2.pdf**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** on the code above U is a global variable that is use to compute the gradient of the lagrangian as well as the update of X. the global variable U is updated during the dual descent. ex below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "G = 0\n",
    "\n",
    "def foo():\n",
    "    global G\n",
    "    G = 10\n",
    "    \n",
    "print(G)\n",
    "foo()\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
