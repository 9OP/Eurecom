{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optim HW 2 : Gradient methods\n",
    "___\n",
    "Martin Guyard\n",
    "\n",
    "**Due date: 20 Nov. 2019**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "___\n",
    "Goal is to minimize unconstrained $f$:\n",
    "\n",
    "$$ f(x_1,x_2,...,x_m) = \\sum_{i=1}^{m} a_1\\cdot(x_i-b_i)^2 + 3 $$\n",
    " \n",
    "$$ x_{i}^{k+1} = x_{i}^{k} - t\\nabla{f(x_{i}^{k})} $$\n",
    "$$ x_{i}^{k+1} = x_{i}^{k} - t\\cdot2\\cdot a_i(x_{i}^{k} - b_i) $$\n",
    "\n",
    "___\n",
    "**Analytical solution**:\n",
    "\n",
    "$f$ is minimum when $\\forall i, x_i = b_i$. Moreover we can solve this problem with gradient descent in one step with:\n",
    "$\\forall i, x_{i}^{k} - t\\cdot2\\cdot a_i(x_{i}^{k} - b_i) = b_i $, means that $\\forall{i},  2\\cdot t \\cdot a_i = 1$.\n",
    "\n",
    "**One trivial solution is $t=0.5$ and $\\forall i, a_i=1$**\n",
    "___\n",
    "**Convergence analysis**:\n",
    "\n",
    "The gradient descent algorithm converge on $t<1/L$ given that $f$ is convex and $L$ Lipschitz.\n",
    "\n",
    "$f$ is $L$ Lipschitz if $\\| \\nabla f(x) - \\nabla f(y)\\|_2 \\leq L\\|x-y\\|_2$\n",
    "\n",
    "\n",
    "$\\|A(X-B) - A(Y-b)\\|_2 \\leq L \\|X-Y\\|_2$\n",
    "\n",
    "$\\|AX - AY\\|_2 \\leq L \\|X-Y\\|_2$\n",
    "\n",
    "$\\|A(X-Y)\\|_2 \\leq L \\|X-Y\\|_2$\n",
    "\n",
    "$max(A)\\|X-Y\\|_2 \\leq L \\|X-Y\\|_2$\n",
    "\n",
    "$L \\geq max(A)$\n",
    "\n",
    "**if $\\forall i, a_i=1$, then the convergence criterium is $L \\geq 1$ and $t \\leq 1$**\n",
    "\n",
    "Gradient (and gradient+backtracking) convergence rate is $O(1/k)$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from random import randrange\n",
    "\n",
    "def norm(K, L=None):\n",
    "    # return euclidean distance between vector K and L\n",
    "    if L:\n",
    "        X = [k-l for k, l  in zip(K, L)]\n",
    "    else:\n",
    "        X = K\n",
    "    return sqrt(sum(x**2 for x in X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, A, B, epsilon=1e-6, step=0.5):\n",
    "    assert type(X) == type(A) == type(B) == list\n",
    "    assert len(X) == len(A) == len(B)\n",
    "    \n",
    "    f_grad = lambda X: [2*a*(x-b) for x, a, b in zip(X, A, B)]\n",
    "    update = lambda X: [x-step*xf for x, xf in zip(X, f_grad(X))]\n",
    "    \n",
    "    X_k = list(X)\n",
    "    X_k1 = update(X_k)\n",
    "    \n",
    "    steps = 0\n",
    "    while norm(X_k, X_k1) >= epsilon:\n",
    "        X_k2 = update(X_k1)\n",
    "        X_k = list(X_k1)\n",
    "        X_k1 = list(X_k2)\n",
    "        steps += 1\n",
    "        \n",
    "    print(\"Gradient descent steps:\", steps)\n",
    "    print(\"[ step size t =\", step, ', convergence criterium 1/t <', max(A),']\\n')\n",
    "    return X_k    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with Backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_bt(X, A, B, epsilon=1e-6, step=0.5, alpha=0.5, beta=0.5):\n",
    "    assert type(X) == type(A) == type(B) == list\n",
    "    assert len(X) == len(A) == len(B)\n",
    "    assert 0 < alpha <= 0.5 and 0 < beta < 1\n",
    "    \n",
    "    f = lambda X: sum([(a*(x-b)**2)+3 for x, a, b in zip(X, A, B)])\n",
    "    f_grad = lambda X: [2*a*(x-b) for x, a, b in zip(X, A, B)]\n",
    "    update = lambda X: [x-step*xf for x, xf in zip(X, f_grad(X))]\n",
    "    \n",
    "    X_k = list(X)\n",
    "    X_k1 = update(X_k)\n",
    "    \n",
    "    steps = 0\n",
    "    while norm(X_k, X_k1) >= epsilon:\n",
    "        while f(X_k1) > f(X_k) - alpha * step * norm(f_grad(X_k)) ** 2:\n",
    "            step *= beta\n",
    "            X_k1 = update(X_k)\n",
    "        else:\n",
    "            X_k2 = update(X_k1)\n",
    "            X_k = list(X_k1)\n",
    "            X_k1 = list(X_k2)\n",
    "        steps += 1\n",
    "    \n",
    "    print('Gradient descent with backtracking steps:', steps)\n",
    "    print('[ step size t =', step, ', alpha =', alpha, ', beta =', beta, ']\\n')\n",
    "    return X_k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Assume that:\n",
    "\n",
    "- $ m = 500 $\n",
    "\n",
    "- $ \\forall i, a_i = 1 $\n",
    "\n",
    "- $ \\forall i, b_i \\sim U(0, 100) $\n",
    "\n",
    "- $ \\forall i, x_i = 0 $ for initial x\n",
    "\n",
    "**As we already analyticaly solved this precise problem, I choose step size t = 1**\n",
    "\n",
    "**The stopping condition is $d(x_{k+1}, x_{k}) \\leq \\varepsilon$ with default $\\varepsilon = 1e^{-6}$ and $d(.)$ the euclidean distance**\n",
    "\n",
    "**Note: The stopping condition could also be X == B or norm(f_grad(X)) <= epsilon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 500\n",
    "X = [0] * m\n",
    "A = [1] * m\n",
    "B = [randrange(0, 100) for i in range(m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent steps: 1\n",
      "[ step size t = 0.5 , convergence criterium 1/t < 1 ]\n",
      "\n",
      "Gradient descent with backtracking steps: 1\n",
      "[ step size t = 0.5 , alpha = 0.5 , beta = 0.5 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step size = 0.5\n",
    "gd_res = gradient_descent(X, A, B)\n",
    "\n",
    "# step size = 0.5, alpha = 0.5, beta = 0.5\n",
    "gd_bt_res = gradient_descent_bt(X, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The convergence speed is the same with step size t=0.5 since this solve the problem analytically as shown above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Assume that:\n",
    "\n",
    "- $ m = 500 $\n",
    "\n",
    "- $ \\forall i, a_i \\sim U(1, 100) $\n",
    "\n",
    "- $ \\forall i, b_i \\sim U(1, 100) $\n",
    "\n",
    "- $ \\forall i, x_i = 0 $ for initial x\n",
    "\n",
    "**I choose step size t=1/100 since max(A) < 100**\n",
    "\n",
    "**The stopping condition is $d(x_{k+1}, x_{k}) \\leq \\varepsilon$ with default $\\varepsilon = 1e^{-6}$ and $d(.)$ the euclidean distance**\n",
    "\n",
    "**Note: The stopping condition could also norm(f_grad(X)) <= epsilon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 500\n",
    "X = [0] * m\n",
    "A = [randrange(1, 100) for i in range(m)]\n",
    "B = [randrange(1, 100) for i in range(m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent steps: 967\n",
      "[ step size t = 0.01 , convergence criterium 1/t < 99 ]\n",
      "\n",
      "Gradient descent with backtracking steps: 1375\n",
      "[ step size t = 0.005 , alpha = 0.5 , beta = 0.5 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step size = 0.5\n",
    "gd_res = gradient_descent(X, A, B, step=1/100)\n",
    "\n",
    "# step size = 0.5, alpha = 0.5, beta = 0.5\n",
    "gd_bt_res = gradient_descent_bt(X, A, B, step=1/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The convergence speed is slower with backtracking as it is taking more steps, however backtracking advantage is not to converge faster, but to always converge, with batcktracking step size can be anything since the backtrack is updating the step size**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "___\n",
    "Goal is to minimize **constrained** $f$:\n",
    "\n",
    "$$ f(x_1,x_2,...,x_m) = \\sum_{i=1}^{m} a_1\\cdot(x_i-b_i)^2 + 3 $$\n",
    "$$\\forall i, x_i \\geq 0 $$\n",
    "$$\\sum_{i=0}^{m} x_i \\leq 100 $$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Anaytical solution\n",
    "\n",
    "This problem has no equalities constraints, the Lagrangian is simply:\n",
    "<br>$L(x, \\lambda) = f(x) - \\lambda h(x)$ where $x=(x_1, ... , x_m)$ and $h(x)=(\\sum_{i=1}^{m} x_i) -100$\n",
    "<br>\n",
    "<br>\n",
    "KKT conditions:\n",
    "- primal feasibility: $\\forall i, x_i \\geq 0$ and $\\sum_{i=1}^m x_i -100 = 0$\n",
    "- dual feasibility: $\\lambda \\geq 0$\n",
    "- complementary slackness: $x_i(2a_i(x_i-b_i)-\\lambda)=0$ and $\\lambda (\\sum_{i=1}^{m} x_i - 100) = 0$\n",
    "\n",
    "\n",
    "<br>\n",
    "We consider $a_i, b_i \\geq 1$\n",
    "<br>\n",
    "<br>\n",
    "With complementary slackness, $xi=b_i+\\frac{\\lambda}{2a_i}\\geq0$\n",
    "<br>\n",
    "\n",
    "**So: $x_i=b_i+\\frac{\\lambda}{2a_ib_i}$**\n",
    "\n",
    "Then $\\lambda (\\sum_{i=1}^{m} x_i - 100) = 0$ implies that either $\\lambda = 0$ or $\\sum_{i=1}^{m} x_i - 100=0$\n",
    "\n",
    "if $\\lambda = 0$ then $x_i=b_i$ but $\\sum_{i=1}^{m} b_i \\leq 100$ might be wrong.\n",
    "\n",
    "My assumption is: $\\sum_{i=1}^{m} x_i - 100=0$\n",
    "\n",
    "So we have: $\\sum_{i=1}^{m} b_i+\\frac{\\lambda}{2a_ib_i} = 100$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**From here our analytical solution is easy to find:**\n",
    "\n",
    "$\\lambda = \\frac{100-\\sum_{i=1}^{m}b_i}{\\sum_{i=1}^{m}\\frac{1}{2a_ib_i}}$\n",
    "<br>\n",
    "$\\forall i, x_i=b_i+\\frac{\\lambda}{2a_ib_i}$\n",
    "<br>\n",
    "Thus $\\forall i, x_i=b_i+\\frac{\\frac{100-\\sum_{i=1}^{m}b_i}{\\sum_{i=1}^{m}\\frac{1}{2a_ib_i}}}{2a_ib_i}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 500\n",
    "X = [0] * m\n",
    "A = [randrange(1, 100) for i in range(m)]\n",
    "B = [randrange(1, 100) for i in range(m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.99999999998366 24785\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def heuristic(X, A, B):\n",
    "    f = lambda X: sum([((a*(x-b)**2)+3) for x, a, b in zip(X, A, B)])\n",
    "    lmbda = (100-sum(B))/(sum([1/(2*a*b) for a, b in zip(A, B)]))\n",
    "    X_k = [b+lmbda/(2*a*b) for a, b in zip(A, B)]\n",
    "    print(sum(X_k), sum(B))\n",
    "    print(f(X) < f(X_k))\n",
    "\n",
    "heuristic(X, A, B)\n",
    "    \n",
    "def dual_ascent(X, A, B, epsilon=1e-6, step=0.5):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dual ascent YT tuto: https://www.youtube.com/watch?v=HOx-fZ01VnY**\n",
    "\n",
    "**exercise: https://bdesgraupes.pagesperso-orange.fr/UPX/Master1/MNM1_corr_doc2.pdf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
